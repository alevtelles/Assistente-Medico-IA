# ü©∫ Assistente Virtual M√©dico com IA - Solu√ß√£o Baseada em RAG

Este aplicativo √© um chatbot m√©dico que utiliza a t√©cnica de RAG (Retrieval Augmented Generation) para fornecer respostas precisas e relevantes a partir de documentos m√©dicos carregados pelo usu√°rio. O sistema realiza uma recupera√ß√£o de informa√ß√µes (retrieval) e as utiliza como contexto para gerar respostas contextualmente mais robustas, minimizando erros e aumentando a assertividade nas respostas m√©dicas.



## üôãüèΩ O que √© RAG
RAG (Retrieval Augmented Generation) √© uma t√©cnica que aprimora modelos de linguagem integrando informa√ß√µes externas relevantes de uma base de dados ou documentos. Isso ajuda a evitar erros de gera√ß√£o (conhecidos como "alucina√ß√µes") e a garantir respostas mais precisas, especialmente em dom√≠nios complexos e especializados como a medicina.

---

## ‚öôÔ∏è Arquitetura

```
User Input
   ‚Üì
Query Embedding ‚Üí Pinecone Vector DB ‚Üê Embedded Chunks ‚Üê Chunking ‚Üê PDF Loader
   ‚Üì
Retrieved Docs
   ‚Üì
     RAG Chain (Groq + LangChain)
   ‚Üì
LLM-generated Answer

```
- Entrada do Usu√°rio: O usu√°rio faz uma consulta ao sistema.
- Embedding da Consulta: A consulta √© convertida em embeddings para facilitar a recupera√ß√£o de documentos relevantes.
- Banco de Dados Vetorial (Pinecone): Armazena vetores de documentos e permite a recupera√ß√£o eficiente de informa√ß√µes.
- RAG Chain: Processa a consulta, acessando o modelo LLaMA3-70B da Groq atrav√©s do LangChain para gerar respostas baseadas no conte√∫do recuperado.
- Resposta Gerada pelo LLM: O modelo gera uma resposta contextualizada utilizando o conhecimento recuperado.


## üìã Funcionalidades

- Carregamento de PDFs M√©dicos: Suporte para upload de documentos m√©dicos em formato PDF (notas, livros, relat√≥rios, etc.).
- Extra√ß√£o Autom√°tica de Texto: O conte√∫do dos PDFs √© extra√≠do automaticamente e fragmentado em blocos sem√¢nticos para facilitar a recupera√ß√£o.
- Embedding de Conte√∫do: O texto extra√≠do √© transformado em embeddings usando Google/BGE para otimizar a recupera√ß√£o de informa√ß√µes.
- Armazenamento de Vetores: Os vetores gerados s√£o armazenados na base de dados vetorial Pinecone, permitindo buscas r√°pidas e precisas.
- Uso do LLaMA3-70B: Utiliza√ß√£o do modelo LLaMA3-70B da Groq para gerar respostas baseadas nos dados recuperados.
- Backend em FastAPI: API desenvolvida com FastAPI para gerenciamento de uploads de documentos e consultas (Q&A).

---

## üöÄ üöÄ Tecnologias Utilizadas


| Tecnologia | Descri√ß√£o |
|------------|-----------|
| Backend | FastAPI (framework ass√≠ncrono para APIs r√°pidas) |
| LLM | API Groq (LLaMA3-70B) |
| Embeddings | Google Generative AI / BGE |
| Vector DB | 	Pinecone (armazenamento de vetores) |
| Framework | LangChain (integra√ß√£o de RAG com LLaMA3-70B) |
| Deploy | Render (opcional para deploy em nuvem)| 

---

## üõ£Ô∏è Rotas


```bash
POST /upload_pdfs/ --- Carregar um ou mais arquivos PDFs

POST /ask/ --- Rota de perguntas

```
- /upload_pdfs/: Endpoint respons√°vel pelo carregamento dos PDFs m√©dicos, realizando a extra√ß√£o e fragmenta√ß√£o autom√°tica do conte√∫do.

- /ask/: Endpoint para enviar uma pergunta ao assistente, que utiliza o conte√∫do carregado para fornecer uma resposta baseada em dados factuais.
